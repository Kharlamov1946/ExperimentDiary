После долгих размышлений и перечитыванию материала, присланного ранее, я понял, что Андрей Жонин и Харламов А. А. предполагают две различные
реализации. О каждой подробнее:

	1. Из аудиозаписи извлекается последовательность векторов MFCC. Так как  дикторы разные и говорят с разной скоростью, а векторы мы берём 
	всегда с одной и той же частотой, применяется Dynamic Time Warping для придачи каждому вектору однозначного смысла (по большей части). 
	При обучении для каждой встреченной фонемы записываются все встреченные ей векторы и находится средний (для экономии памяти можно 
	записывать только средний, так как остальные в дальнейшем не используются). Если к фонеме пытаются приписать вектор, сильно отличающийся от
	среднего, можно создавать ещё один кластер с той же фонемой и обучать её отдельно. Данные сохраняются, обучение завершено. При распознавании мы
	находим между входящим вектором последовательности и каждым из фонемных средних векторов квадрат Евклидового расстояния. Относим наш вектор
	к тому типу, расстояние с которым было наименьшим. Ещё можно использовать Марковские скрытые сети (об этом Андрей не говорил).
	
	2. Из аудиозаписи извлекается последовательность векторов MFCC. К этой последовательности мы добавляем в самое начало (n-1) векторов,
	состоящих из k нулей, где n - регистр сдвига, а k - размерность вектора. Затем при обучении мы смещаемся на 1, начиная с начала и 
	захватывая однновременно n векторов. Для каждой встреченной последовательности векторов (матрицы n*k) создаём нейрон (если последовательность
	замечена впервые). Что такое нейрон? Нейрон обладает определённой последовательностью, на которую он откликается, и набором 
	фонетических значков, каждый из которых имеет счётчик. Также у нейрона есть порог, который изначально равен нулю (или очень большому числу, 
	но нули весят меньше). Что такое порог? Это значение, на которое может отличаться входящая последовательность с дальнейшей активацией нейрона.
	При обучении мы постепенно или увеличиваем порог (если он равен 0), или уменьшаем (если равен изнач. очень большому числу) до тех пор, пока 
	выбранное нами предложение не станет распознаваться полностью или единственно. Звучит хорошо, но как реализовать процесс обучения на практике - 
	не представляю. Например, как выбирать последовательность, порог которой мы будем изменять? Есть ещё вариант записывать новый нейрон, определением
	которого является значок, а не последовательность. Тогда этот нейрон будет иметь один значок (без счётчика) и набор всех последовательностей, 
	отнесённых к данной фонеме. А порогом назначить наибольшую дисперсию, встреченную при обучении. Но тогда тоже придётся использовать DTW, так
	как мы обучаем нейрон на последовательностях от разных дикторов, а значит, они будут сильно искажать математическое ожидание (что приведёт к
	совпадению откиков разных нейронов, что нас не устраивает).
	
Первый способ уже сейчас популярен при распознавании речи, работает быстро и не требует особых затрат памяти, но не факт, что подойдёт для нашей 
настоящей постановки (с электроэнцефалографом). Второй чисто в теории должен работать (по крайней мере, мне так кажется при отсутствии опыта),
но выглядит убийственно громоздким. В любом случае, я думаю, что самостоятельно два способа в один я совместить не смогу, поэтому руководителям
необходимо переговорить на эту тему между собой. Ну и я послушаю заодно.